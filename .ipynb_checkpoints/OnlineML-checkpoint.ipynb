{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [River - Python library for online machine learning](https://github.com/online-ml/river). \n",
    "\n",
    "It is the result of a merger between creme and scikit-multiflow. River's ambition is to be the go-to library for doing machine learning on streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Learning vs. Online Learning\n",
    "\n",
    "### Batch Learning\n",
    "\n",
    "Basic approach is the following:\n",
    "\n",
    "1. Collect some data, i.e. features $X$ and labels $Y$\n",
    "2. Train a model on $(X, Y)$, i.e. generate a function $f(X) \\approx Y$\n",
    "3. Save the model somewhere\n",
    "4. Load the model to make predictions\n",
    "\n",
    "Some drawbacks of batch learning are:\n",
    "\n",
    "- Models have to be retrained from scratch with new data\n",
    "- Models always \"lag\" behind\n",
    "- With increasing data, the comp. requirements increase\n",
    "- Batch models are **static** \n",
    "- Some locally developed features are not available in production/real-time\n",
    "\n",
    "Batch learning is popular mainly since it is taught at university, it is the main source of competitions on Kaggle, there are **libraries available** and one may achieve higher levels of accuracy in a direct comparison to online learning.\n",
    "\n",
    "Available libraries: [sklearn](https://scikit-learn.org/stable/), [pytorch](https://pytorch.org/), [tensorflow](https://tensorflow.org), etc. \n",
    "\n",
    "---\n",
    "\n",
    "### Online Learning\n",
    "\n",
    "Video sources: [Max Halford](https://www.youtube.com/watch?v=P3M6dt7bY9U), [Andrew Ng](https://www.youtube.com/watch?v=dnCzy_XKGbA)\n",
    "\n",
    "Literature sources: [Comprehensive Survey](https://arxiv.org/pdf/1802.02871.pdf)\n",
    "\n",
    "Different names for the same thing: **Incremental Learning**, *Sequential Learning*, **Iterative Learning**, *Out-of-core Learning*\n",
    "\n",
    "Basic features:\n",
    "\n",
    "- Data comes from a stream, i.e. in sequential order\n",
    "- Models learn 1 observation at a time\n",
    "- Observations do not have to be stored \n",
    "- Features and labels are dynamic \n",
    "- Models can dynamically adapt to new patterns in the data\n",
    "\n",
    "Available libraries: [river](https://github.com/online-ml/river), [vowpal wabbit](https://vowpalwabbit.org/)\n",
    "\n",
    "Usefull applications in\n",
    "\n",
    "- Time series forecasting \n",
    "- Spam filters and recommender systems\n",
    "- IoT\n",
    "- Basically, **anything event based** \n",
    "\n",
    "Algorithmic scheme of online learning:\n",
    "    <div style=\"background-color:rgba(0, 0, 0, 0.0670588); padding:5px 0;font-family:monospace;\">\n",
    "    <font color = \"red\">Forever do</font><br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; Get $(x,y)$ corresponding to new data.    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; Update $\\Theta$ using $(x,y)$ with SGD step:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Theta_j := \\Theta_j - \\gamma \\nabla L$.<br>\n",
    "    </div>\n",
    "\n",
    "\n",
    "Major Drawbacks:\n",
    "\n",
    "- [Catastrophic inference](https://www.wikiwand.com/en/Catastrophic_interference): NN abruptly forgets what it has learned, first brought to the attention in 1989\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Deep Learning ODL\n",
    "\n",
    "based on [paper](https://www.ijcai.org/Proceedings/2018/0369.pdf)\n",
    "\n",
    "The main challenges for DNNs in the online learning setting are\n",
    "\n",
    "- vanishing gradients: DNNs loose the abiltiy to learn because gradients tend towards 0\n",
    "- diminishing feature reuse\n",
    "- saddle points\n",
    "- immense number of parameters to optimize\n",
    "- internal covariate shifts\n",
    "\n",
    "In the paper above, the authors extend the classic DNN such that each layer $h^i$ acts as output layer. The final prediction $F_t$ for data $t$ of the model is then given as linear combination of the predictions $f^i(x)$ of the individual layers. For parameter optimization, they introduce the so-called **Hedge Backpropagation**. Both is visualized in the paper in the following figure, in which $h^i$ is the respective layer of the DNN, i.e. nonlinear activation function of a linear combination of the inputs, $f^i$ is the respecitve prediction of the layer and $\\alpha_i$ is the respective weight.\n",
    "\n",
    "![Hedge Backpropagation](HBP.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Catastrophic Inference](https://www.wikiwand.com/en/Catastrophic_interference)\n",
    "\n",
    "Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ratcliff (1990). \n",
    "\n",
    "It is a radical manifestation of the **'sensitivity-stability' dilemma** or the **'stability-plasticity' dilemma**. Specifically, these problems refer to the challenge of making an artificial neural network that is sensitive to new information but not disrupted by. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former (LuT) remains completely stable in the presence of new information but lacks the ability to generalize, i.e. to infer general principles from new inputs. On the other hand, neural networks like the standard backpropagation network can generalize to unseen inputs, but they are very sensitive to new information. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. \n",
    "\n",
    "The main cause of catastrophic interference seems to be overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation, each input tends to create changes in the weights of many of the nodes. Catastrophic forgetting occurs because when many of the weights where \"knowledge is stored\" are changed, it is unlikely for prior knowledge to be kept intact. During sequential learning, the inputs become mixed, with the new inputs being superimposed on top of the old ones. Another way to conceptualize this is by visualizing learning as a movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network could possess. When a network first learns to represent a set of patterns, it finds a point in the weight space that allows it to recognize all of those patterns. However, when the network then learns a new set of patterns, it will move to a place in the weight space for which the only concern is the recognition of the new patterns. To recognize both sets of patterns, the network must find a place in the weight space suitable for recognizing both the new and the old patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online GPR\n",
    "\n",
    "1. [Link to Uni Cluj](http://www.cs.ubbcluj.ro/~csatol/SOGP/thesis/Gaussian_Process.html)\n",
    "2. [Local GPR for Real Time Online Model Learning and Control - 2008](https://papers.nips.cc/paper/2008/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf) - MPI (Max Planck Institute) for Biological Cybernetics, [Matthias Seeger](https://mseeger.github.io/)\n",
    "3. [Incremental Local Gaussian Regression - 2014](https://papers.nips.cc/paper/2014/file/6aca97005c68f1206823815f66102863-Paper.pdf) - USC & MPI for Intelligent Systems, [Stefan Schaal](http://www-clmc.usc.edu/~sschaal/)\n",
    "4. [Online Sparse Gaussian Process Training with Input Noise - 2016](https://arxiv.org/pdf/1601.08068v1.pdf) - TU Delft & University of Uppsala, [Michael Verhaegen](https://www.tudelft.nl/en/3me/about/departments/delft-center-for-systems-and-control/people/professors/profdrir-m-verhaegen-michel/)\n",
    "5. [A unifying view of sparse approximate GPR - 2005](https://www.quinonero.net/Publications/quinonero-candela05a.pdf) - MPI for Biological Cybernetics, Candela & Rasmussen\n",
    "6. [Consistent Online GPR without the Sample Complexity Bottleneck](https://arxiv.org/pdf/2004.11094v1.pdf) - \n",
    "\n",
    "### 2. Local GPR for Real Time Online Model Learning and Control\n",
    "\n",
    "Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up **standard Gaussian process regression (GPR)** with **local GP models** (LGP). The training **data is partitioned in local regions**, for each an individual GP model is trained.\n",
    "The **prediction** for a query point is performed **by weighted estimation using nearby local models**. The proposed method achieves online learning and prediction in real-time. \n",
    "\n",
    "The main computational cost of this algorithm is $\\mathcal{O}(N^3)$ for inverting the local covariance matrix, where $N$ presents the number of data points in a local model. Furthermore, we can **control the\n",
    "complexity by limiting the number of data points in a local model**. Since the number of local data points increases continuously over time, we can adhere to comply with this limit by **deleting old data\n",
    "point as new ones are included**. Insertion and deletion of data points can be decided by evaluating the information gain of the operation. The cost for inverting the local covariance matrix can be\n",
    "further reduced, as we need only to update the full inverse matrix once it is computed. The update can be efficiently performed in a stable manner using **rank-one update** which has a complexityof $\\mathcal{O}(N^2)$.\n",
    "\n",
    "The following figure shows results from the paper given in miliseconds.\n",
    "\n",
    "<img src=\"LGPR_prediction_time.PNG\" width=\"400\">\n",
    "\n",
    "### 3. Incremental Local Gaussian Regression\n",
    "\n",
    "Gaussian (process) regression provides a generative model with rather black-box automatic parameter tuning, but it has\n",
    "high computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR (locally weighted regression). Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.\n",
    "\n",
    "The paper proposed a top-down approach to probabilistic localized regression. Local Gaussian Regression decouples inference over M local models, resulting in efficient and principled updates for all\n",
    "parameters, including local distance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Online Learning in Neural Networks -- David Saad 1998](https://aitonline-my.sharepoint.com/personal/tobias_glueck_ait_ac_at/Documents/Microsoft%20Teams-Chatdateien/Saad%20-%20On-Line%20Learning%20in%20Neural%20Networks.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online GAMs\n",
    "\n",
    "there seems to be no literature to the online applicaiton of GAMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('thesis': conda)",
   "language": "python",
   "name": "python37664bitthesisconda4e23a8f720bf476ebd0f5fe3f9ef6962"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
